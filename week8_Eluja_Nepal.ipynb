{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "GYlITMpA8RSu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class CustomDecisionTree:\n",
        "  def __init__(self, max_depth=None):\n",
        "    \"\"\"\n",
        "    Initializes the decision tree with the specified maximum depth.\n",
        "    Parameters:\n",
        "    max_depth (int, optional): The maximum depth of the tree. If None, the tree is expanded until all\n",
        "    leaves are pure or contain fewer than the minimum samples required to split.\n",
        "    \"\"\"\n",
        "    self.max_depth = max_depth\n",
        "    self.tree = None\n",
        "  def fit(self, X, y):\n",
        "    \"\"\"\n",
        "    Trains the decision tree model using the provided training data.\n",
        "    Parameters:\n",
        "    X (array-like): Feature matrix (n_samples, n_features) for training the model.\n",
        "    y (array-like): Target labels (n_samples,) for training the model.\n",
        "    \"\"\"\n",
        "    self.tree = self._build_tree(X, y)\n",
        "  def _build_tree(self, X, y, depth=0):\n",
        "    \"\"\"\n",
        "    Recursively builds the decision tree by splitting the data based on the best feature and threshold\n",
        "    .\n",
        "    Parameters:\n",
        "    X (array-like): Feature matrix (n_samples, n_features) for splitting.\n",
        "    y (array-like): Target labels (n_samples,) for splitting.\n",
        "    depth (int, optional): Current depth of the tree during recursion.\n",
        "    Returns:\n",
        "    dict: A dictionary representing the structure of the tree, containing the best feature index,\n",
        "    threshold, and recursive tree nodes.\n",
        "    \"\"\"\n",
        "    num_samples, num_features = X.shape\n",
        "    unique_classes = np.unique(y)\n",
        "    # Stopping conditions: pure node or reached max depth\n",
        "    if len(unique_classes) == 1:\n",
        "      return {'class': unique_classes[0]}\n",
        "    if num_samples == 0 or (self.max_depth and depth >= self.max_depth):\n",
        "      return {'class': np.bincount(y).argmax()}\n",
        "    # Find the best split based on Information Gain (using Entropy)\n",
        "    best_info_gain = -float('inf')\n",
        "    best_split = None\n",
        "    for feature_idx in range(num_features):\n",
        "      thresholds = np.unique(X[:, feature_idx])\n",
        "      for threshold in thresholds:\n",
        "        left_mask = X[:, feature_idx] <= threshold\n",
        "        right_mask = ~left_mask\n",
        "        left_y = y[left_mask]\n",
        "        right_y = y[right_mask]\n",
        "        info_gain = self._information_gain(y, left_y, right_y)\n",
        "        if info_gain > best_info_gain:\n",
        "          best_info_gain = info_gain\n",
        "          best_split = {\n",
        "            'feature_idx': feature_idx,\n",
        "            'threshold': threshold,\n",
        "            'left_y': left_y,\n",
        "            'right_y': right_y,\n",
        "    }\n",
        "\n",
        "    if best_split is None:\n",
        "      return {'class': np.bincount(y).argmax()}\n",
        "\n",
        "    # Recursively build the left and right subtrees\n",
        "    left_tree = self._build_tree(X[best_split['left_y']], best_split['left_y'], depth + 1)\n",
        "    right_tree = self._build_tree(X[best_split['right_y']], best_split['right_y'], depth + 1)\n",
        "\n",
        "    return {'feature_idx': best_split['feature_idx'], 'threshold': best_split['threshold'],'left_tree': left_tree, 'right_tree': right_tree}\n",
        "\n",
        "  def _information_gain(self, parent, left, right):\n",
        "    \"\"\"\n",
        "    Computes the Information Gain between the parent node and the left/right child nodes.\n",
        "    Parameters:\n",
        "    parent (array-like): The labels of the parent node.\n",
        "    left (array-like): The labels of the left child node.\n",
        "    right (array-like): The labels of the right child node.\n",
        "    Returns:\n",
        "    float: The Information Gain of the split.\n",
        "    \"\"\"\n",
        "    parent_entropy = self._entropy(parent)\n",
        "    left_entropy = self._entropy(left)\n",
        "    right_entropy = self._entropy(right)\n",
        "    # Information Gain = Entropy(parent) - (weighted average of left and right entropies)\n",
        "    weighted_avg_entropy = (len(left) / len(parent)) * left_entropy + (len(right) / len(parent)) * right_entropy\n",
        "\n",
        "    return parent_entropy - weighted_avg_entropy\n",
        "\n",
        "# def _entropy(self, y):\n",
        "#   \"\"\"\n",
        "#   Computes the entropy of a set of labels.\n",
        "#   Parameters:\n",
        "#   y (array-like): The labels for which entropy is calculated.\n",
        "#   Returns:\n",
        "#   float: The entropy of the labels.\n",
        "#   \"\"\"\n",
        "#   # Calculate the probability of each class\n",
        "#   class_probs = np.bincount(y) / len(y)\n",
        "#   # Compute the entropy using the formula: -sum(p * log2(p))\n",
        "#   return -np.sum(class_probs * np.log2(class_probs + 1e-9)) # Added small epsilon to avoid log(0)\n",
        "# def predict(self, X):\n",
        "#   \"\"\"\n",
        "#   Predicts the target labels for the given test data based on the trained decision tree.\n",
        "#   Parameters:\n",
        "#   X (array-like): Feature matrix (n_samples, n_features) for prediction.\n",
        "#   Returns:\n",
        "#   list: A list of predicted target labels (n_samples,).\n",
        "#   \"\"\"\n",
        "#   return [self._predict_single(x, self.tree) for x in X]\n",
        "\n",
        "# def _predict_single(self, x, tree):\n",
        "#   \"\"\"\n",
        "#   Recursively predicts the target label for a single sample by traversing the tree.\n",
        "#   Parameters:\n",
        "#   x (array-like): A single feature vector for prediction.\n",
        "#   tree (dict): The current subtree or node to evaluate.\n",
        "#   Returns:\n",
        "#   int: The predicted class label for the sample.\n",
        "#   \"\"\"\n",
        "#   if 'class' in tree:\n",
        "#     return tree['class']\n",
        "#   feature_val = x[tree['feature_idx']]\n",
        "#   if feature_val <= tree['threshold']:\n",
        "#     return self._predict_single(x, tree['left_tree'])\n",
        "#   else:\n",
        "#     return self._predict_single(x, tree['right_tree'])\n",
        "\n",
        "  def _entropy(self, y):\n",
        "    \"\"\"\n",
        "    Computes the entropy of a set of labels.\n",
        "    \"\"\"\n",
        "    class_probs = np.bincount(y) / len(y)\n",
        "    return -np.sum(class_probs * np.log2(class_probs + 1e-9))\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Predicts the target labels for the given test data.\n",
        "    \"\"\"\n",
        "    return [self._predict_single(x, self.tree) for x in X]\n",
        "\n",
        "  def _predict_single(self, x, tree):\n",
        "    \"\"\"\n",
        "    Recursively predicts the target label for a single sample.\n",
        "    \"\"\"\n",
        "    if 'class' in tree:\n",
        "      return tree['class']\n",
        "    if x[tree['feature_idx']] <= tree['threshold']:\n",
        "      return self._predict_single(x, tree['left_tree'])\n",
        "    else:\n",
        "      return self._predict_single(x, tree['right_tree'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Split into training and test sets (80% training, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Train the custom decision tree\n",
        "custom_tree = CustomDecisionTree(max_depth=3)\n",
        "custom_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_custom = custom_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Custom Decision Tree Accuracy: {accuracy_custom:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qekdn0G9-tBi",
        "outputId": "54a6a631-ffe7-41fb-8691-28083df0a394"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Decision Tree Accuracy: 0.8000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Scikit-learn decision tree\n",
        "sklearn_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "sklearn_tree.fit(X_train, y_train)\n",
        "# Predict on the test set\n",
        "y_pred_sklearn = sklearn_tree.predict(X_test)\n",
        "# Calculate accuracy\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "print(f\"Scikit-learn Decision Tree Accuracy: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwl8PX6EWbD",
        "outputId": "b20c9b99-f12e-4cde-cae8-91aeeeaacea9"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scikit-learn Decision Tree Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy Comparison:\")\n",
        "print(f\"Custom Decision Tree: {accuracy_custom:.4f}\")\n",
        "print(f\"Scikit-learn Decision Tree: {accuracy_sklearn:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQtLzaD-EY6c",
        "outputId": "cbcd1871-6a63-4b8f-b29b-917d13b1a9eb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison:\n",
            "Custom Decision Tree: 0.8000\n",
            "Scikit-learn Decision Tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**task 3**"
      ],
      "metadata": {
        "id": "zmphyTxfIVcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X = wine.data      # features (13 columns)\n",
        "y = wine.target    # target class (0,1,2)\n"
      ],
      "metadata": {
        "id": "6Niaxyb6IUOK"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "tGqHbhSnIUQZ"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import f1_score\n"
      ],
      "metadata": {
        "id": "8OtARt9XIUT1"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Decision Tree\n",
        "dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate F1 score\n",
        "f1_dt = f1_score(y_test, y_pred_dt, average='macro')\n",
        "print(f\"Decision Tree F1 Score: {f1_dt:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTpVan4wIUuL",
        "outputId": "2c844b1a-e32f-4cc3-dab5-c42fb91ad07f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree F1 Score: 0.9432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate F1 score\n",
        "f1_rf = f1_score(y_test, y_pred_rf, average='macro')\n",
        "print(f\"Random Forest F1 Score: {f1_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9NjZkmmIUxx",
        "outputId": "cc1f184f-2ab7-43c6-eaaa-6beb53498197"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest F1 Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nF1 Score Comparison:\")\n",
        "print(f\"Decision Tree: {f1_dt:.4f}\")\n",
        "print(f\"Random Forest: {f1_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjYDGPB6LuUo",
        "outputId": "e281d571-6e41-43c4-c926-fc757729899f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "F1 Score Comparison:\n",
            "Decision Tree: 0.9432\n",
            "Random Forest: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [2, 3, 4, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf,\n",
        "                           param_grid=param_grid,\n",
        "                           scoring='f1_macro',   # maximize F1 score\n",
        "                           cv=5,                 # 5-fold cross-validation\n",
        "                           n_jobs=-1)            # use all CPU cores\n",
        "\n",
        "# Fit on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Best F1 score from cross-validation\n",
        "print(\"Best F1 Score (CV):\", grid_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VI2qLWUkLuYB",
        "outputId": "b5a854ee-956d-487b-808e-8f52fbe479ea"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Best F1 Score (CV): 0.9788068209740036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Decision Tree Regressor\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Select feature matrix X and target y\n",
        "X_reg = X[:, 1:]   # all features except 'alcohol'\n",
        "y_reg = X[:, 0]    # predict 'alcohol'\n",
        "\n",
        "# Split into train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "dt_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_dt = dt_reg.predict(X_test_reg)\n",
        "print(\"Decision Tree Regressor R2:\", r2_score(y_test_reg, y_pred_dt))\n",
        "print(\"Decision Tree Regressor MSE:\", mean_squared_error(y_test_reg, y_pred_dt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJFIAW7zRDAo",
        "outputId": "da40112a-14af-4643-bf95-61356b03e12a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor R2: 0.5120320319194656\n",
            "Decision Tree Regressor MSE: 0.2913343474021956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Random Forest Regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
        "rf_reg.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_rf = rf_reg.predict(X_test_reg)\n",
        "print(\"Random Forest Regressor R2:\", r2_score(y_test_reg, y_pred_rf))\n",
        "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test_reg, y_pred_rf))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFA1IR0cRHL0",
        "outputId": "6bcb237f-4dd3-48b6-a267-e1adb255b738"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Regressor R2: 0.7504662893678093\n",
            "Random Forest Regressor MSE: 0.14898055917039113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter Tuning using RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_dist = {\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'max_depth': [2, 3, 4, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=rf_reg,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=10,            # number of random combinations\n",
        "                                   scoring='r2',         # maximize R2 score\n",
        "                                   cv=5,                 # 5-fold CV\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Best parameters and score\n",
        "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
        "print(\"Best R2 Score (CV):\", random_search.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw4uMgm8RQBB",
        "outputId": "331e1081-55af-41fa-9b29-ec92fdc601f7"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'n_estimators': 50, 'min_samples_split': 2, 'max_depth': 3}\n",
            "Best R2 Score (CV): 0.5244990271923053\n"
          ]
        }
      ]
    }
  ]
}